{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BasicAttention(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(BasicAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, c):\n",
    "        e = T.bmm(c, q.transpose(1, 2))\n",
    "        ai = F.softmax(e, dim=-1)\n",
    "        \n",
    "        a = T.bmm(ai, q)\n",
    "        b = T.cat([c, a], dim=-1)\n",
    "        \n",
    "        out = self.dropout(b)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 600, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = T.zeros([8, 600, 256])\n",
    "Q = T.zeros([8, 100, 256])\n",
    "\n",
    "D = D.cuda()\n",
    "Q = Q.cuda()\n",
    "\n",
    "i = BasicAttention(0.2).cuda()\n",
    "o = i(Q, D)\n",
    "\n",
    "o.shape#, Q_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BasicAttention(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super(BasicAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, c):\n",
    "        e = T.bmm(c, q.transpose(1, 2))\n",
    "        ai = F.softmax(e, dim=-1)\n",
    "        \n",
    "        a = T.bmm(ai, q)\n",
    "        b = T.cat([c, a], dim=-1)\n",
    "        out = self.dropout(b)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class BiAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BiAttention, self).__init__()\n",
    "        self.wsim = nn.Linear(6*hidden_size, 1, bias=False)\n",
    "    \n",
    "    def forward(self, q, c):\n",
    "        q_len = q.shape[1]\n",
    "        c_len = c.shape[1]\n",
    "\n",
    "        q_t = q.unsqueeze(1).repeat(1, c_len, 1, 1)\n",
    "        c_t = c.unsqueeze(2).repeat(1, 1, q_len, 1)\n",
    "        q_elementwise_c = T.mul(q_t, c_t)\n",
    "        \n",
    "        matrix = T.cat([q_t, c_t, q_elementwise_c], 3)\n",
    "        S = self.wsim(matrix)\n",
    "        S = S.squeeze()\n",
    "        \n",
    "        context2query = T.bmm(F.softmax(S, dim=-1), q)\n",
    "\n",
    "        b = F.softmax(T.max(S, 2)[0], dim=-1)\n",
    "        query2context = T.bmm(b.unsqueeze(1), c).repeat(1, c_len, 1)\n",
    "        \n",
    "        contextatten = c.mul(context2query)\n",
    "        queryatten = c.mul(query2context)\n",
    "        \n",
    "        return T.cat([c, context2query, contextatten, queryatten], 2)\n",
    "\n",
    "class CoAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CoAttention, self).__init__()\n",
    "        self.Wqj = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        self.c0 = nn.Parameter(T.rand(2*hidden_size,))\n",
    "        self.q0 = nn.Parameter(T.rand(2 * hidden_size, ))\n",
    "        self.bilstm = nn.LSTM(6*hidden_size, 2*hidden_size, batch_first=True, bidirectional=True)\n",
    "    \n",
    "    def forward(self, q, c):\n",
    "        b, _, l = q.shape\n",
    "        q_vec = self.q0.unsqueeze(0).expand(b, l).unsqueeze(1)\n",
    "        qj = T.cat([q, q_vec], dim=1)\n",
    "        Qj = F.tanh(self.Wqj(qj))\n",
    "        \n",
    "        c_vec = self.c0.unsqueeze(0).expand(b, l).unsqueeze(1)\n",
    "        Dj = T.cat([c, c_vec], dim=1)\n",
    "\n",
    "        L = T.bmm(Dj, Qj.transpose(1, 2))\n",
    "        AQ = F.softmax(L, dim=2)\n",
    "        AD = F.softmax(L.transpose(1, 2), dim=2)\n",
    "        \n",
    "        CQ = T.bmm(AQ.transpose(1, 2), Dj)        \n",
    "        CD = T.bmm(AD.transpose(1, 2), T.cat([Qj, CQ], dim=-1))\n",
    "        \n",
    "        U, _ = self.bilstm(T.cat([Dj, CD], dim=-1))\n",
    "        \n",
    "        return U[:,:-1,:]\n",
    "    \n",
    "def tile(x, dim, num_tile):\n",
    "    shape = x.shape\n",
    "    repeat_dim = [1]*(len(shape)+1)\n",
    "    repeat_dim[dim] = num_tile\n",
    "    return x.unsqueeze(dim).repeat(*repeat_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlptorch",
   "language": "python",
   "name": "nlptorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
