{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from embedding import CharacterConvEmbedding\n",
    "from attention import BasicAttention, BiAttention, CoAttention\n",
    "from network import EmbedText, Encoder, DynamicPointingDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_args = {'vocab_size': 70, 'embedding_dim': 8, 'padding_idx':10}\n",
    "char_conv_args = {'char_embed_dim':8, 'embedding_dim':100, 'kernel_sizes':[2, 3, 4 ,5], 'num_filters':50}\n",
    "word_args = {'vocab_size':40000, 'embedding_dim':100, 'padding_idx':10}\n",
    "\n",
    "c_char = T.zeros([32, 600, 50], dtype=T.long)\n",
    "c_word = T.zeros([32, 600], dtype=T.long)\n",
    "q_char = T.zeros([32, 50, 50], dtype=T.long)\n",
    "q_word = T.zeros([32, 50], dtype=T.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.FloatTensor torch.cuda.FloatTensor torch.cuda.FloatTensor torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "embedtext = EmbedText(word_args, char_args, shared=False)\n",
    "qw, cw, qc, cc = embedtext(q_word, c_word, q_char, c_char)\n",
    "qw = qw.cuda()\n",
    "cw = cw.cuda()\n",
    "qc = qc.cuda()\n",
    "cc = cc.cuda()\n",
    "\n",
    "print (qw.type(), cw.type(), qc.type(), cc.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.FloatTensor torch.cuda.FloatTensor\n",
      "torch.Size([32, 50, 400]) torch.Size([32, 600, 400])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(word_args, char_conv_args, hidden_dim=200, shared=True, use_cuda=True)\n",
    "\n",
    "q_lstm, c_lstm = encoder(qw, cw, qc, cc)\n",
    "\n",
    "print (q_lstm.type(), c_lstm.type())\n",
    "print (q_lstm.shape, c_lstm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 600, 800])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = CoAttention(200, use_cuda=True)\n",
    "o = i(q_lstm, c_lstm)\n",
    "\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn = DynamicPointingDecoder(hidden_dim=400, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 599,  599,  599,  599,  599,  599,  599,  599,  599,  599,\n",
       "          599,  599,  599,  599,  599,  599,  599,  599,  599,  599,\n",
       "          599,  599,  599,  599,  599,  599,  599,  599,  599,  599,\n",
       "          599,  599], device='cuda:0'),\n",
       " tensor([ 599,  599,  599,  599,  599,  599,  599,  599,  599,  599,\n",
       "          599,  599,  599,  599,  599,  599,  599,  599,  599,  599,\n",
       "          599,  599,  599,  599,  599,  599,  599,  599,  599,  599,\n",
       "          599,  599], device='cuda:0'),\n",
       " [[tensor(1.00000e-02 *\n",
       "          [[ 5.5623,  5.6655,  5.6609,  ...,  6.1996,  6.5787,  7.5521],\n",
       "           [ 5.8287,  5.8563,  5.8453,  ...,  6.4582,  6.9495,  7.7475],\n",
       "           [ 5.9363,  6.0408,  6.1307,  ...,  6.6884,  7.1567,  8.0594],\n",
       "           ...,\n",
       "           [ 6.4139,  6.6795,  6.9410,  ...,  7.1870,  7.3650,  8.2088],\n",
       "           [ 6.5386,  6.6904,  6.9493,  ...,  7.2008,  7.3920,  8.0821],\n",
       "           [ 6.5748,  6.8221,  6.9110,  ...,  7.2317,  7.3989,  8.0391]], device='cuda:0'),\n",
       "   tensor([[ 0.0882,  0.0887,  0.0901,  ...,  0.0975,  0.1046,  0.1192],\n",
       "           [ 0.0926,  0.0934,  0.0953,  ...,  0.1002,  0.1045,  0.1171],\n",
       "           [ 0.0957,  0.0979,  0.0998,  ...,  0.1052,  0.1081,  0.1160],\n",
       "           ...,\n",
       "           [ 0.0989,  0.1046,  0.1076,  ...,  0.1092,  0.1113,  0.1156],\n",
       "           [ 0.0990,  0.1035,  0.1063,  ...,  0.1088,  0.1105,  0.1141],\n",
       "           [ 0.0980,  0.1012,  0.1020,  ...,  0.1054,  0.1064,  0.1099]], device='cuda:0')],\n",
       "  [tensor(1.00000e-02 *\n",
       "          [[ 6.3750,  6.3714,  6.3790,  ...,  6.5383,  6.8218,  7.8305],\n",
       "           [ 6.5207,  6.7110,  6.6701,  ...,  6.9226,  7.1802,  8.1853],\n",
       "           [ 6.6284,  6.7908,  6.8311,  ...,  7.0991,  7.3681,  8.2763],\n",
       "           ...,\n",
       "           [ 6.9792,  7.0430,  7.2505,  ...,  7.2358,  7.3239,  8.2001],\n",
       "           [ 7.0895,  7.1295,  7.3084,  ...,  7.2771,  7.4638,  8.1454],\n",
       "           [ 7.1063,  7.2849,  7.4761,  ...,  7.5820,  7.6974,  8.0495]], device='cuda:0'),\n",
       "   tensor([[ 0.0926,  0.0925,  0.0928,  ...,  0.0987,  0.1051,  0.1185],\n",
       "           [ 0.0959,  0.0967,  0.0984,  ...,  0.1016,  0.1057,  0.1172],\n",
       "           [ 0.0985,  0.1007,  0.1022,  ...,  0.1026,  0.1049,  0.1159],\n",
       "           ...,\n",
       "           [ 0.1007,  0.1045,  0.1062,  ...,  0.1075,  0.1093,  0.1138],\n",
       "           [ 0.0991,  0.1033,  0.1051,  ...,  0.1071,  0.1088,  0.1132],\n",
       "           [ 0.0981,  0.1016,  0.1037,  ...,  0.1059,  0.1073,  0.1096]], device='cuda:0')]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dn(o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlptorch",
   "language": "python",
   "name": "nlptorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
